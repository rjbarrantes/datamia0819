MACHINE LEARNING

recommender systems
  used to determine recommendations
  based on user or product comparisons
    distance metrics used to measure closeness
    recommendations are based on closeness of person and strength of recommendation

survival analysis
  probability of survival
    car engine
    human
    insurance policy
    churn
  need three fields
    column = probability of event (1/0)
    row = datetime (equidistant)
    color = groupings
  fit === learning from data

time series
  static data === independent observations
    y depends on coefficients and x's
    y = b0 + b1x + ...
  time-ordered data === dependent observations
    y depends on previous y
      can be confirmed using a lagplot (scatterplot of yt and yt-1)
    yt = b0 + b1 yt-1
    auto-regression
    non-systematic === random, error term
      cannot be modeled
    systematic === non-random
      things model takes into account when making predictions
      average (by time interval)
      seasonality
      trend (big-picture)
      noise (errors)

supervised (inferential)
  training and test set
  always has x and y (target)
  prediction
    numeric
      regression
        bi/multivariate
        k-nn
          nonparametric
          average k nearest points
          piecewise 
        SVR
    categorical
      logistic regression
        bi/multinomial
        SVC
    accuracy
      R^2, R^2adj, MSE, RMSE
      MAE, MAD, MAPE usually used for time-series
      based on differences between actual and predicted targets (residuals)
  validation
    hyperparameter tuning
    adjust parameters to get best mix wrt accuracy
    has training, validation, AND test sets
  cross-validation
    run train/test multiple times to get an average
    average is more robust and less varied
  classification
    logistic regression
      provides sigmoid function
      maps probability to a classification
      probability threshold usually .5
      for more categories - create more ranges
    naive bayes
      bayes law = P(A|B) = P(B|A) * P(A) / P(B)
    accuracy
      confusion matrix =
                  predicted
                  y      n     
        actual  y True+  False-
                n False+ True-
      accuracy score = (True+ + True-)/ total
        rate of match between actual and predicted targets
        if data imbalanced, accuracy score might overestimate
        need enough number of observations in each cell (70/30 is good cutoff)
        balanced accuracy = more robust
          includes weights based on sample sizes
          larger size results in smaller weight
          better to balance sizes in collection
      precision = True+ / (True+ + False+)
        true-positive given predicted positive
      recall = True+ / (True+ + False-)
        true-positive given actual positive
      roc plot = plot True+ and False+ rate
        good if plot above diagonal line
        auc (integral of roc) = larger is good
unsupervised (descriptive)
  only has x (no target)
  used for clustering/segmentation
  distances between observations in nd space
    euclidean distance (c^2 = a^2 + b^2)
      c
    manhattan distance
      a + b
      moving in axis grids
    cosine distance
  no measure of accuracy
    need k
      k-means (numerical)
        k centroids placed randomly
        k centroids eventually move to closest k clusters of data
          can one-hot encode, but categorical better with k-mode
        limitation is selecction of k
      k-mode (categorical)
      k-prototype
      gmm = gaussian mixture model
        fitting to normal distributions
        expectation maximization algorithims
    no k needed
      hierarchical clustering
      agglomerative clustering
        bottom-up
          each point is a cluster
          clusters form with closest points
          keeps going algorithmically
      dbscan = density-based spatial clustering of applications with noise
        specify minimum points and minimum distance (ε) instead of k
        more robust to outliers
  evaluating clusters
    pca
    som = self-organizing maps
    tsne = t-distributed stochastic neighbor embedding
      perplexity should be between 5 and 50
    silhouette score
      (b - a) / max(a, b)
      a = within-cluster variation
      b = between-cluster variation
      between -1 and 1 where closer to 1 is better
      similar to k-means optimization, so can lead to overfitting
        having a hold-out test set of data can help with this issue
      favors convex clusters 
        this is fine for data that is amenable to k-means style clustering
          for many datasets it will give an artificially low score
    adjusted rand score
      have to have true clusters
        usually generated by hand
    optimal clusters
      plot kmeans inertia by number of clusters
      optimal at the point where the elbow or inflection point changes significantly
  
decision trees
  can be used for classification and regression
  decisions at each interior node (binary)
    each observation is sent down the tree until it hits a bottom node (tree leaf)
    y value depends on which segment of x the observation falls into
    with more than one, observation goes through decisions on both variables
    regression decisions are based on xi < c
    classification decsisions are based on xi : c
  ensemble methods
    bagging
      boostrap aggregate
      take b bootstrap samples from training data each same size as training data
        with replacement
      fit large tree to each bootstrap sample
        b trees
      combine results from each of the b trees to get an overall prediction
    boosting
  random forests
    several trees taken into account
    b = bootstrap samples
    m = number of variables to sample
      common is m = p**.5
      p = dimenion of x
    maximizes impact of important features

ai
  symbolic
    older concept of ai
  automated
    machine learning
    deep learning
      can take tb of data and provide results
      neural networks (nn)
        big data analysis
          simple algorithms cannot handle
          only deep learning algorithms
      xgb = descent gradient-boosting

deep learning
  input -> activation function (hidden layers) -> output
  every neuron has some weights associated with it
  back propagation
    loss function = predicted - actual
      no loss function = feed-forward nn
    optimizer adjusts weights to minimize loss function
      adam
      sgd
    learning rate (α)
  layers
    input = one neuron per feature
    hidden = can vary layers and size of layers
      each neuron has summation (linear combination of weights) and activation
      bias + w1I1 + w2I2 + ...
        bias is constant and defined to trigger or not trigger neuron
      activation functions
        sigmoid
        relu = rectified linear unit
        tanh = hyperbolic tangent
        step function
      fully connected = every neuron connected to others in forward direction
    output
      regression = 1
      classification = k
  curse of dimensionality = p (features) >> n
    will not perform well
  tenors
    0d = scalar
    1d = array
    2d = matrix
    nd = tensor of rank n

natural language processing (nlp)
  sentiment analysis
    positive = 1
    negative = -1
  steps
    tokenize = split string to list of tokens (or grams)
    stopwords = remove punctuations and clean meaningless words
    stemming (or lemmatization) = convert words to root form
    can make bag of words = frequency of words
    vectorization = transformation into array and strings --> 0/1s
  tf = term frequency
  idf = inverse document frequency